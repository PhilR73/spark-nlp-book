{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDo1n_94sHCx"
   },
   "source": [
    "<div data-type=\"part\">\n",
    "<h1>Part 1: Basics</h1>\n",
    "\n",
    "<section data-type=\"chapter\">\n",
    "<h1>Chapter 1: Getting Started</h1>\n",
    "\n",
    "<section data-type=\"sect1\">\n",
    "<h1>Introduction</h1>\n",
    "\n",
    "<p>This book is about using Spark NLP to build natural language processing applications (NLP). Spark NLP is an NLP library built on top of Apache Spark.&nbsp;In this book I'll cover how to use Spark NLP, as well fundamental natural language processing topics. Hopefully, at the end of this book you'll have a new software tool for working with natural language, Spark NLP, and a suite of techniques and some understanding of why these techniques work.</p>\n",
    "<span style=\"letter-spacing: 0.18px;\">Let's begin by talking about the structure of this book. In the first part, we'll go over the technologies and techniques we'll be using with Spark NLP throughout this book. After that we'll talk about the building blocks of&nbsp;</span><em style=\"letter-spacing: 0.18px;\"><strong>N</strong>atural&nbsp;<strong>L</strong>anguage&nbsp;<strong>P</strong>rocessing</em><span style=\"letter-spacing: 0.18px;\">&nbsp;(NLP). Finally, we'll talk about NLP applications and sytems.</span>\n",
    "\n",
    "<p>When working on an application that requires NLP, there are three perspectives you should keep in mind: the software developer's perspective, the linguist's perspective, and the data scientist's perspective. The software developer's perspective focuses on what your application needs to do; this grounds the work in terms of the product you want to create. The linguist's perspective focuses on what it is in the data that you want to extract. The data scientist's perspective focuses on how you can extract the information you need from your data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nftkpjh9taxR"
   },
   "source": [
    "<p>Here is a more detailed overview of the book</p>\n",
    "\n",
    "<ul>\n",
    "\t<li>PART I: INTRODUCTION\n",
    "\t<ul>\n",
    "\t\t<li><em>Getting Started</em> covers setting up your environment so you can follow along with the examples and exercises in the book</li>\n",
    "\t\t<li><em>Natural Language Basics</em> is a survey of some of the linguistic concepts that help in understanding why NLP techniques work, and how to use NLP techniques get the information you need from language.</li>\n",
    "\t\t<li><em>NLP on Apache Spark</em> is an introduction to Apache Spark and, most germane, the Spark NLP library.</li>\n",
    "\t\t<li><em>Deep Learning Basics</em> is a survey of some of the deep learning concepts that we'll be using in this book. This book is <em>not </em>a tutorial on deep learning, but we'll try and explain these techniques when necessary.</li>\n",
    "\t</ul>\n",
    "\t</li>\n",
    "\t<li>PART II: BUILDING BLOCKS\n",
    "\t<ul>\n",
    "\t\t<li><em>Processing Words</em> covers the classic text processing techniques. Since NLP applications generally require a pipeline of transformations, understanding the early steps well is a necessity.</li>\n",
    "\t\t<li><em>Information Retrieval</em>&nbsp;covers the basic concepts of search engines. Not only is this a classic example of an application that uses text, but many NLP techniques used in other kinds of applications ultimately come from information retrieval.</li>\n",
    "\t\t<li><em>Classification and Regression</em> covers some well established techniques of using text features for classification and regression tasks.</li>\n",
    "\t\t<li><em>Sequence Modeling</em> introduces techniques used in modeling natural language text data as sequences. Since natural language is a sequence, these techniques are fundamental.</li>\n",
    "\t\t<li><em>Information Extraction</em> shows how we can extract facts and relationships from text.</li>\n",
    "\t\t<li><em>Topic Modeling</em> demonstrates techniques for finding topics in documents. Topic modeling is a great way to explore text.</li>\n",
    "\t\t<li><em>Embeddings</em> discusses one of the most popular modern techniques for creating features from text.</li>\n",
    "\t</ul>\n",
    "\t</li>\n",
    "\t<li>PART III: APPLICATIONS\n",
    "\t<ul>\n",
    "\t\t<li><em>Sentiment Analysis &amp; Emotion Detection</em> covers some basic applications that require identifying the sentiment of the author of a text, e.g. was a movie review positive or negative.</li>\n",
    "\t\t<li><em>Building Knowledge Graphs</em> explores creating an ontology, a collection of facts and relationships organized in a graph-like manner, from a corpus.</li>\n",
    "\t\t<li><em>Semantic Search</em> goes deeper into what can be done to improve a search engine. Improving is not just about improving the ranker, it's also about facilitating the user with features like facets.</li>\n",
    "\t\t<li><em>Conversational Chatbots</em> demonstrates how to create a chatbot - this is a fun and interesting application. These kinds of applications are becoming more and more popular.</li>\n",
    "\t\t<li><em>Object Character Recognition (OCR)</em> introduces converting text stored as images to text data. Not all texts are stored as text data. Handwriting and old texts are examples of texts we may receive as images. Sometimes, we also have to deal with non-handwritten text stored in images like PDF images and scans of printed documents.&nbsp;</li>\n",
    "\t</ul>\n",
    "\t</li>\n",
    "\t<li>PART IV: BUILDING NLP SYSTEMS\n",
    "\t<ul>\n",
    "\t\t<li><em>Supporting Multiple Languages</em> explores topics that an application creator should consider when preparing to work with multiple languages.</li>\n",
    "\t\t<li><em>Human Labeling: Collecting Quality Training Data</em> covers ways to use Humans to gather data about texts. Being able to efficiently use Humans to augment data can make an otherwise impossible project feasible.</li>\n",
    "\t\t<li><em>Training and Publishing NLP Models</em> covers creating models, Spark NLP pipelines and TensorFlow graphs, and publishing them for use in production.</li>\n",
    "\t\t<li><em>Scaling and Performance Optimization</em> discusses some of the performance concerns developers should keep in mind when designing a system that uses text.</li>\n",
    "\t\t<li><em>Operating NLP Systems in Production</em> covers quality and monitoring concerns that are unique to NLP applications.</li>\n",
    "\t</ul>\n",
    "\t</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcswudzcteLA"
   },
   "source": [
    "<h3>Other Tools</h3>\n",
    "\n",
    "<p>In addition to Spark NLP, Apache Spark, and TensorFlow, we'll make use of a number of other tools</p>\n",
    "\n",
    "<ul>\n",
    "\t<li><strong>Python</strong>&nbsp;is one of the most popular programming languages used in data science. Although Spark NLP is implemented in Scala, we will be demonstrating its use through Python. Learn more at&nbsp;<a href=\"https://www.python.org/\">https://www.python.org/</a></li>\n",
    "\t<li><strong>Anaconda </strong>is an open source distribution of Python (and R which we are not using). It is maintained by Anaconda, Inc. who also offer an enterprise platform, and training courses. We'll use the Anaconda package manager,&nbsp;<code>conda</code>, to create our environment. Learn more at <a href=\"https://www.anaconda.com/\">https://www.anaconda.com/</a></li>\n",
    "\t<li><strong>Jupyter Notebook</strong> is a tool for executing code in the browser. Jupyter Notebook also allows you to write markdown, and display visualizations all in the browser. In fact, this book was written as Jupyter Notebooks before being converted to a publishable format. Jupyter Notebook is maintained by Project Jupyter which is a non-profit dedicated to supporting interactive data science tools. Learn more at <a href=\"https://jupyter.org/\">https://jupyter.org/</a></li>\n",
    "\t<li><strong>Docker </strong>is a tool for easily creating virtual machines, often referred to as <em>containers</em>. We'll use Docker as an alternative installation tool to setting up conda. It is maintained by Docker, Inc. Learn more at <a href=\"https://www.docker.com/\">https://www.docker.com/</a></li>\n",
    "</ul>\n",
    "</section>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SVWQy70yth_Y"
   },
   "source": [
    "<section data-type=\"sect1\">\n",
    "<h1>Setting up your environment</h1>\n",
    "\n",
    "<p>In this book, almost every chapter has exercises, so it is useful to make sure that the environment is working at the beginning. We'll use Jupyter notebooks in this book, and the kernel we'll use is the baseline Python 3.6 kernel. The instructions here use Continuum's Anaconda to set up a Python virtual environment.</p>\n",
    "\n",
    "<p>You can also use the `johnsnowlabs/spark-nlp-workshop` docker image for the necessary environment.</p>\n",
    "\n",
    "<p>These instructions were created from the set up process for Ubuntu. There are additional set up instructions online at the project's github page.</p>\n",
    "\n",
    "<section data-type=\"sect2\">\n",
    "<h2>Prerequisites</h2>\n",
    "\n",
    "<ol>\n",
    "\t<li>Anaconda\n",
    "\t<ul>\n",
    "\t\t<li>To set up Anaconda, follow the instructions at - <a href=\"https://docs.anaconda.com/anaconda/install/\">https://docs.anaconda.com/anaconda/install/</a></li>\n",
    "\t</ul>\n",
    "\t</li>\n",
    "\t<li>Apache Spark\n",
    "\t<ul>\n",
    "\t\t<li>To set up Apache Spark, follow the instructions at - <a href=\"https://spark.apache.org/docs/latest/\">https://spark.apache.org/docs/latest/</a></li>\n",
    "\t\t<li>Make sure that <code>SPARK_HOME</code> is set to the location of your Apache Spark installation.\n",
    "\t\t<ul>\n",
    "\t\t\t<li>If you are on Linux or macOS, you can put&nbsp;<code>export SPARK_HOME=\"/path/to/spark\"</code></li>\n",
    "\t\t\t<li>If you are on Windows, you can use System Properties to set an environment variable named <code>SPARK_HOME</code> to&nbsp;&nbsp;<code>\"/path/to/spark\"</code></li>\n",
    "\t\t</ul>\n",
    "\t\t</li>\n",
    "\t\t<li>This was written on Apache Spark 2.4</li>\n",
    "\t</ul>\n",
    "\t</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Optional</strong>: Set up a password for your Jupyter notebook server - <a href=\"https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup\">https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup</a></p>\n",
    "</section>\n",
    "\n",
    "<section data-type=\"sect2\">\n",
    "<h2>Starting Apache Spark</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rs5h9VdvtoNN"
   },
   "source": [
    "`$ echo $SPARK_HOME`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBvvevZetqXf"
   },
   "source": [
    "`/path/to/your/spark/installation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEitx0bpttd_"
   },
   "source": [
    "`$ spark-shell`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJMzuYqItvqF"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.prope\n",
    "rties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setL\n",
    "ogLevel(newLevel).\n",
    "...\n",
    "Spark context Web UI available at localhost:4040\n",
    "Spark context available as 'sc' (master = local[*], app id = ...).\n",
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0\n",
    "_102)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdJsfELqtzmr"
   },
   "source": [
    "<section data-type=\"sect2\">\n",
    "<h2>Checking out the code</h2>\n",
    "\n",
    "<ol>\n",
    "\t<li>Go to github repo for this project -&nbsp;<a href=\"https://github.com/alexander-n-thomas/spark-nlp-book\">https://github.com/alexander-n-thomas/spark-nlp-book</a></li>\n",
    "\t<li>Check out the code, run the following code examples in a terminal\n",
    "\t<ol>\n",
    "\t\t<li>Clone the repo\n",
    "\t\t<div class=\"highlight\">\n",
    "\t\t<pre data-code-language=\"bash\">\n",
    "git clone https://github.com/alexander-n-thomas/spark-nlp-book.git\n",
    "</pre>\n",
    "\t\t</div>\n",
    "\t\t</li>\n",
    "\t\t<li>Create the conda environment - this will take a while\n",
    "\t\t<div class=\"highlight\">\n",
    "\t\t<pre data-code-language=\"bash\">\n",
    "conda env create -f environment.yml\n",
    "</pre>\n",
    "\t\t</div>\n",
    "\t\t</li>\n",
    "\t\t<li>Activate the new environment\n",
    "\t\t<div class=\"highlight\">\n",
    "\t\t<pre data-code-language=\"bash\">\n",
    "<span class=\"nb\">source</span> activate spark-nlp-book\n",
    "</pre>\n",
    "\t\t</div>\n",
    "\t\t</li>\n",
    "\t\t<li>Create the kernel for this environment\n",
    "\t\t<div class=\"highlight\">\n",
    "\t\t<pre data-code-language=\"bash\">\n",
    "ipython kernel install --user --name<span class=\"o\">=</span>sparknlpbook\n",
    "</pre>\n",
    "\t\t</div>\n",
    "\t\t</li>\n",
    "\t\t<li>Launch the notebook server\n",
    "\t\t<div class=\"highlight\">\n",
    "\t\t<pre data-code-language=\"bash\">\n",
    "jupyter notebook\n",
    "</pre>\n",
    "\t\t</div>\n",
    "\t\t</li>\n",
    "\t\t<li>Go to your notebook page at&nbsp;<a href=\"http://localhost:8888/\">localhost:8888</a></li>\n",
    "\t</ol>\n",
    "\t</li>\n",
    "</ol>\n",
    "</section>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sjX3PueEt3_o"
   },
   "source": [
    "<section data-type=\"sect1\">\n",
    "<h2>Getting Familiar with Apache Spark</h2>\n",
    "\n",
    "<p>Now that we're all set up, let's start using Spark NLP! We will be using the <em>20 Newsgroups</em> data set from the University of California Irvine Machine Learning Repository. The data can be found at <a href=\"https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\">https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups</a>. For this first example we use the mini_newsgroups data set found at <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/mini_newsgroups.tar.gz\">https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/mini_newsgroups.tar.gz</a>. Download this tar file and extract it into the data folder for this project.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzgTx-7OuEnm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p:\\Dev\\Personal\\FinTech\\spark-nlp-book\\jupyter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file data already exists.\n",
      "Error occurred while processing: data.\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "dgBCSO9Fr-M5",
    "outputId": "928e0b31-0fe4-4496-b883-09f36a96fe97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-04-09 18:30:04--  https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/mini_newsgroups.tar.gz\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1860687 (1.8M) [application/x-httpd-php]\n",
      "Saving to: 'mini_newsgroups.tar.gz'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  2% 54.4K 32s\n",
      "    50K .......... .......... .......... .......... ..........  5% 49.9K 33s\n",
      "   100K .......... .......... .......... .......... ..........  8% 43.9K 34s\n",
      "   150K .......... .......... .......... .......... .......... 11% 38.9K 35s\n",
      "   200K .......... .......... .......... .......... .......... 13% 27.1K 39s\n",
      "   250K .......... .......... .......... .......... .......... 16% 45.3K 37s\n",
      "   300K .......... .......... .......... .......... .......... 19% 73.4K 33s\n",
      "   350K .......... .......... .......... .......... .......... 22% 47.9K 32s\n",
      "   400K .......... .......... .......... .......... .......... 24% 27.5K 33s\n",
      "   450K .......... .......... .......... .......... .......... 27% 52.2K 31s\n",
      "   500K .......... .......... .......... .......... .......... 30% 62.6K 29s\n",
      "   550K .......... .......... .......... .......... .......... 33% 63.3K 27s\n",
      "   600K .......... .......... .......... .......... .......... 35% 64.1K 25s\n",
      "   650K .......... .......... .......... .......... .......... 38% 40.4K 25s\n",
      "   700K .......... .......... .......... .......... .......... 41% 63.5K 23s\n",
      "   750K .......... .......... .......... .......... .......... 44% 45.2K 22s\n",
      "   800K .......... .......... .......... .......... .......... 46% 44.3K 21s\n",
      "   850K .......... .......... .......... .......... .......... 49% 19.3K 21s\n",
      "   900K .......... .......... .......... .......... .......... 52% 28.4K 21s\n",
      "   950K .......... .......... .......... .......... .......... 55% 32.3K 20s\n",
      "  1000K .......... .......... .......... .......... .......... 57% 37.4K 19s\n",
      "  1050K .......... .......... .......... .......... .......... 60% 47.0K 17s\n",
      "  1100K .......... .......... .......... .......... .......... 63% 25.5K 17s\n",
      "  1150K .......... .......... .......... .......... .......... 66% 28.8K 16s\n",
      "  1200K .......... .......... .......... .......... .......... 68% 44.3K 14s\n",
      "  1250K .......... .......... .......... .......... .......... 71% 40.8K 13s\n",
      "  1300K .......... .......... .......... .......... .......... 74% 28.6K 12s\n",
      "  1350K .......... .......... .......... .......... .......... 77% 52.2K 11s\n",
      "  1400K .......... .......... .......... .......... .......... 79% 62.4K 9s\n",
      "  1450K .......... .......... .......... .......... .......... 82% 76.4K 8s\n",
      "  1500K .......... .......... .......... .......... .......... 85% 63.0K 6s\n",
      "  1550K .......... .......... .......... .......... .......... 88% 51.5K 5s\n",
      "  1600K .......... .......... .......... .......... .......... 90% 77.4K 4s\n",
      "  1650K .......... .......... .......... .......... .......... 93% 77.7K 3s\n",
      "  1700K .......... .......... .......... .......... .......... 96% 62.4K 2s\n",
      "  1750K .......... .......... .......... .......... .......... 99% 23.0K 0s\n",
      "  1800K .......... .......                                    100% 1.28M=43s\n",
      "\n",
      "2022-04-09 18:30:48 (42.3 KB/s) - 'mini_newsgroups.tar.gz' saved [1860687/1860687]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/mini_newsgroups.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7DAiInZuIsG"
   },
   "outputs": [],
   "source": [
    "! tar xzf mini_newsgroups.tar.gz -C ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "kFrFUZ-KuCjE",
    "outputId": "7ed09c79-3498-4d6a-f4d3-ab80a818678c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "! ls ./data/mini_newsgroups/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qj7Y1qkfuRhX"
   },
   "source": [
    "<section data-type=\"sect2\">\n",
    "<h2>Starting Apache Spark with Spark NLP</h2>\n",
    "\n",
    "<p>There are many ways we can use Apache Spark from Jupyter notebooks. We could use a specialized kernel, but I generally prefer using a simple kernel. Fortunately, Spark NLP gives us an easy way to start up.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TV1sDtRJuvbc"
   },
   "outputs": [],
   "source": [
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uz-C2n0EuxgD"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "up1WmOGzuyyL"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sK_RGMRJu1ei"
   },
   "outputs": [],
   "source": [
    "packages = ','.join([\n",
    "    \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\",\n",
    "])\n",
    "\n",
    "log_dir = os.path.join(os.getcwd(),'..','var','log').replace('\\\\','/')\n",
    "log_url_dir = \"file:///\" + log_dir\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "spark_conf = SparkConf()\n",
    "spark_conf = spark_conf.setAppName('spark-nlp-book-p1c1')\n",
    "spark_conf = spark_conf.setAppName('master[*]')\n",
    "spark_conf = spark_conf.set(\"spark.jars.packages\", packages)\n",
    "spark_conf = spark_conf.set(\"spark.driver.memory\",\"3G\")\n",
    "spark_conf = spark_conf.set(\"spark.executor.memory\", \"3G\")\n",
    "spark_conf = spark_conf.set(\"spark.driver.maxResultSize\", \"0\")\n",
    "spark_conf = spark_conf.set(\"spark.kryoserializer.buffer.max\", \"1000M\")\n",
    "spark_conf = spark_conf.set(\"spark.eventLog.enabled\", \"true\")\n",
    "spark_conf = spark_conf.set(\"spark.eventLog.dir\", log_url_dir)\n",
    "spark_conf = spark_conf.set(\"spark.history.fs.logDirectory\", log_url_dir)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVt6KFbOvE2-"
   },
   "source": [
    "<section data-type=\"sect2\">\n",
    "<h2>Loading &amp;&nbsp;viewing data in Apache Spark</h2>\n",
    "\n",
    "<p>Let's look at how we can load data data with Apache Spark, and then some ways we can view the data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAY3MrravEDW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mini_newsgroups_path = os.path.join('data', 'mini_newsgroups', '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9ioOnEHvIES"
   },
   "outputs": [],
   "source": [
    "texts = spark.sparkContext.wholeTextFiles(mini_newsgroups_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zCuJ64fkvJQD"
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('filename', StringType()),\n",
    "    StructField('text', StringType()),\n",
    "])\n",
    "texts_df = spark.createDataFrame(texts, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "colab_type": "code",
    "id": "ML6wzPFIvKGi",
    "outputId": "ec487011-7d41-43b3-e8bf-81a615db3f9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            filename|                text|\n",
      "+--------------------+--------------------+\n",
      "|file:/p:/Dev/Pers...|Path: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Newsgroups: alt.a...|\n",
      "|file:/p:/Dev/Pers...|Path: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Xref: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Path: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Newsgroups: alt.a...|\n",
      "|file:/p:/Dev/Pers...|Newsgroups: alt.a...|\n",
      "|file:/p:/Dev/Pers...|Xref: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Path: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Newsgroups: alt.a...|\n",
      "|file:/p:/Dev/Pers...|Xref: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Newsgroups: alt.a...|\n",
      "|file:/p:/Dev/Pers...|Path: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Newsgroups: alt.a...|\n",
      "|file:/p:/Dev/Pers...|Newsgroups: alt.a...|\n",
      "|file:/p:/Dev/Pers...|Path: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Path: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Path: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Xref: cantaloupe....|\n",
      "|file:/p:/Dev/Pers...|Xref: cantaloupe....|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4jH71Oa2vMlM"
   },
   "source": [
    "<p>Looking at the data is important in any data science project. When working with structured data, especially numerical data, it is common to explore data with aggregates. This is necessary since data sets are large, and looking at a small number of examples can easily misrepresent the data. Natural language data complicates this. On one hand, Humans are really good at interpreting language, on the other, Humans are also really good at jumping to conclusions and making hasty generalizations. So we still have the problem of creating a representitive summary for large data sets. We'll talk about some techniques to do this in the chapters discussing topic modeling and embeddings.</p>\n",
    "\n",
    "<p>For now, let's talk about ways we can look at a small amount of data in&nbsp;<code>DataFrame</code>s. As you can see in the above code example, we can show the output of a&nbsp;<code>DataFrame</code>&nbsp;using&nbsp;<code>.show()</code>. Let's look at the arguments</p>\n",
    "\n",
    "<ol>\n",
    "\t<li><code>n</code>&nbsp;Number of rows to show.</li>\n",
    "\t<li><code>truncate</code>&nbsp;If set to True, truncate strings longer than 20 chars by default. If set to a number greater than one, truncates long strings to length&nbsp;<code>truncate</code>&nbsp;and align cells right.</li>\n",
    "\t<li><code>vertical</code>&nbsp;If set to True, print output rows vertically (one line per column value).</li>\n",
    "</ol>\n",
    "\n",
    "<p>Let's try using some of these arguments&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "1AOKnfxGvK5C",
    "outputId": "16110135-5910-4c14-f17a-9962e4df7043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------------------------------------------\n",
      " filename | file:/p:/Dev/Personal/FinTech/spark-nlp-book/jupyter/data/mini_newsgroups/alt.atheism/51127          \n",
      " text     | Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!ho... \n",
      "-RECORD 1--------------------------------------------------------------------------------------------------------\n",
      " filename | file:/p:/Dev/Personal/FinTech/spark-nlp-book/jupyter/data/mini_newsgroups/alt.atheism/51310          \n",
      " text     | Newsgroups: alt.atheism\\nPath: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.... \n",
      "-RECORD 2--------------------------------------------------------------------------------------------------------\n",
      " filename | file:/p:/Dev/Personal/FinTech/spark-nlp-book/jupyter/data/mini_newsgroups/alt.atheism/53539          \n",
      " text     | Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!newsserv... \n",
      "-RECORD 3--------------------------------------------------------------------------------------------------------\n",
      " filename | file:/p:/Dev/Personal/FinTech/spark-nlp-book/jupyter/data/mini_newsgroups/alt.atheism/53336          \n",
      " text     | Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53336 talk.religion.misc:83720 talk.origins:40943\\nNe... \n",
      "-RECORD 4--------------------------------------------------------------------------------------------------------\n",
      " filename | file:/p:/Dev/Personal/FinTech/spark-nlp-book/jupyter/data/mini_newsgroups/alt.atheism/53212          \n",
      " text     | Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!bogus.su... \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts_df.show(n=5, truncate=100, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1NUn_TTvswt"
   },
   "source": [
    "<p>The&nbsp;<code>.show()</code>&nbsp;method is good for a quick view of data, but if the data is complicated, doesn't work as well. In the Jupyter environment, there are some special integrations with Pandas, and Pandas&nbsp;<code>DataFrame</code>s are displayed a little nicer. Here's an example.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "rX2iqojovPqr",
    "outputId": "8187c75a-286a-4065-d017-372afe774ac6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...</td>\n",
       "      <td>Newsgroups: alt.atheism\\nPath: cantaloupe.srv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...   \n",
       "1  file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...   \n",
       "2  file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...   \n",
       "3  file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...   \n",
       "4  file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...   \n",
       "\n",
       "                                                text  \n",
       "0  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....  \n",
       "1  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....  \n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  \n",
       "3  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...  \n",
       "4  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mnLJ0R87vTrC"
   },
   "source": [
    "<p>Notice the use of&nbsp;<code>.limit()</code>. The&nbsp;<code>.toPandas()</code>&nbsp;method pulls the Spark&nbsp;<code>DataFrame</code>&nbsp;into memory to create a Pandas&nbsp;<code>DataFrame</code>. Converting to Pandas can also be useful for using tools available in the Python, since Pandas&nbsp;<code>DataFrame</code>&nbsp; is widely supported in the Python ecosystem.</p>\n",
    "\n",
    "<p>For other types of visualizations, we'll primarily use the Python libraries matplotlib and seaborn. In order to use these libraries we will need to create Pandas&nbsp;<code>DataFrame</code>s, so we will either aggregate or sample Spark&nbsp;<code>DataFrame</code>s into a manageable size.</p>\n",
    "</section>\n",
    "</section>\n",
    "\n",
    "<section data-type=\"sect1\">\n",
    "<h1>Hello World with Spark NLP</h1>\n",
    "\n",
    "<p>We have some data, so let's use Spark NLP to process it. First, let's extract the newsgroup name from the filename. We can see newsgroup as the last folder in the filename.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IVvEUkfNvRCa"
   },
   "outputs": [],
   "source": [
    "texts_df = texts_df.withColumn(\n",
    "    'newsgroup', \n",
    "    fun.split('filename', '/').getItem(8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "K6j8wD41vVCJ",
    "outputId": "86c012fa-4150-480b-e1fc-3f251b54cb49"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>newsgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....</td>\n",
       "      <td>mini_newsgroups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...</td>\n",
       "      <td>Newsgroups: alt.atheism\\nPath: cantaloupe.srv....</td>\n",
       "      <td>mini_newsgroups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>mini_newsgroups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...</td>\n",
       "      <td>mini_newsgroups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "      <td>mini_newsgroups</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...   \n",
       "1  file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...   \n",
       "2  file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...   \n",
       "3  file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...   \n",
       "4  file:/p:/Dev/Personal/FinTech/spark-nlp-book/j...   \n",
       "\n",
       "                                                text        newsgroup  \n",
       "0  Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv....  mini_newsgroups  \n",
       "1  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....  mini_newsgroups  \n",
       "2  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  mini_newsgroups  \n",
       "3  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...  mini_newsgroups  \n",
       "4  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  mini_newsgroups  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eBoZeLSDuP7j"
   },
   "outputs": [],
   "source": [
    "newsgroup_counts = texts_df.groupBy('newsgroup').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "id": "Ai5EGb2Gv0zg",
    "outputId": "33d05c90-687e-45fb-ac59-c8e5359217eb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "newsgroup_counts.plot(kind='bar', figsize=(10, 5))\n",
    "plt.xticks(\n",
    "    ticks=range(len(newsgroup_counts)), \n",
    "    labels=newsgroup_counts['newsgroup']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Un6_jhrFv5pp"
   },
   "source": [
    "<p><a class=\"list-padding\" href=\"https://atlas.oreilly.com/alex_thomas/nlp-with-spark-nlp/editor/master/images/1.1_newsgroup_counts.jpg\">&nbsp; 1.1_newsgroup_counts.jpg</a></p>\n",
    "\n",
    "<p>Because the mini_newsgroups data set is a subset of the 20Newsgroups data set, we have the same number of documents in each newsgroup. Now, let's use the&nbsp;<a href=\"https://nlp.johnsnowlabs.com/components.html#BasicPipeline\"><code>BasicPipeline</code></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7fl7XCoDv201"
   },
   "outputs": [],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMY8nXAfv8JE"
   },
   "source": [
    "<p>The&nbsp;<code>explain_document_ml</code>&nbsp;is a pretrained pipeline that we can use to process text with a simple pipeline that performs basic processing steps. In order to undetstand what the&nbsp;<code>explain_document_ml</code>&nbsp;is doing, it is necessary to give a brief description of what the annotators are. An annotator is a representation of a specific NLP technique. We will go more in depth when we get to the&nbsp;<em>NLP on Apache Spark</em>&nbsp;chapter.</p>\n",
    "\n",
    "<p><span style=\"letter-spacing: 0.01em;\">The annotators work on a document which is the text, associated metadata, and any previously discovered annotations. This design helps annotators re-use work of previous annotators. The downside, is that it is more complex than libraries like NLTK which are un-coupled collections of NLP functions.</span></p>\n",
    "\n",
    "<p>The&nbsp;<code>explain_document_ml</code>&nbsp;has one&nbsp;<code>Transformer</code>&nbsp;and four annotators</p>\n",
    "\n",
    "<ol>\n",
    "\t<li><code>DocumentAssembler</code>: a&nbsp;<code>Transformer</code>&nbsp;that creates a column that contains documents</li>\n",
    "\t<li><code>Sentence Segmenter</code>: an annotator that produces the sentences of the document</li>\n",
    "\t<li><code>Tokenizer</code>: an annotator that produces the tokens of the sentences</li>\n",
    "\t<li><code>SpellChecker</code>: an annotator that produces the spelling-corrected tokens</li>\n",
    "\t<li><code>Stemmer</code>: an annotator that produces the stems of the tokens</li>\n",
    "\t<li><code>Lemmatizer</code>: an annotator that produces the lemmas of the tokens.</li>\n",
    "\t<li><code>POS Tagger</code>: an annotator that produces the parts of speech of the associated tokens</li>\n",
    "</ol>\n",
    "\n",
    "<p>There are some new terms introduced here that we'll discuss more in upcoming chapters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "Gvi3B14Zv7Tz",
    "outputId": "3e33cc90-3fee-408f-b0e0-dcb8d94ede3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.getDownloadSize.\n: java.lang.NoClassDefFoundError: org/json4s/package$MappingException\r\n\tat org.json4s.ext.EnumNameSerializer.deserialize(EnumSerializer.scala:53)\r\n\tat org.json4s.Formats$$anonfun$customDeserializer$1.applyOrElse(Formats.scala:66)\r\n\tat org.json4s.Formats$$anonfun$customDeserializer$1.applyOrElse(Formats.scala:66)\r\n\tat scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)\r\n\tat scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)\r\n\tat scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)\r\n\tat org.json4s.Formats$.customDeserializer(Formats.scala:66)\r\n\tat org.json4s.Extraction$.customOrElse(Extraction.scala:775)\r\n\tat org.json4s.Extraction$.extract(Extraction.scala:454)\r\n\tat org.json4s.Extraction$.extract(Extraction.scala:56)\r\n\tat org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:22)\r\n\tat com.johnsnowlabs.util.JsonParser$.parseObject(JsonParser.scala:28)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceMetadata$.parseJson(ResourceMetadata.scala:109)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceMetadata$$anonfun$readResources$1.applyOrElse(ResourceMetadata.scala:138)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceMetadata$$anonfun$readResources$1.applyOrElse(ResourceMetadata.scala:137)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat scala.collection.Iterator$$anon$13.next(Iterator.scala:593)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:184)\r\n\tat scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:47)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toList(TraversableOnce.scala:350)\r\n\tat scala.collection.TraversableOnce.toList$(TraversableOnce.scala:350)\r\n\tat scala.collection.AbstractIterator.toList(Iterator.scala:1431)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceMetadata$.readResources(ResourceMetadata.scala:137)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceMetadata$.readResources(ResourceMetadata.scala:132)\r\n\tat com.johnsnowlabs.client.aws.AWSGateway.getMetadata(AWSGateway.scala:78)\r\n\tat com.johnsnowlabs.nlp.pretrained.S3ResourceDownloader.downloadMetadataIfNeed(S3ResourceDownloader.scala:62)\r\n\tat com.johnsnowlabs.nlp.pretrained.S3ResourceDownloader.resolveLink(S3ResourceDownloader.scala:68)\r\n\tat com.johnsnowlabs.nlp.pretrained.S3ResourceDownloader.getDownloadSize(S3ResourceDownloader.scala:145)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.getDownloadSize(ResourceDownloader.scala:445)\r\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader$.getDownloadSize(ResourceDownloader.scala:585)\r\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.getDownloadSize(ResourceDownloader.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.ClassNotFoundException: org.json4s.package$MappingException\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\t... 51 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33844\\235327311.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPretrainedPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'explain_document_ml'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mP:\\Dev\\Personal\\FinTech\\spark-nlp-book\\.env\\win\\lib\\site-packages\\sparknlp\\pretrained.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, lang, remote_loc, parse_embeddings, disk_location)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisk_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdisk_location\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResourceDownloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloadPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipelineModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisk_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mP:\\Dev\\Personal\\FinTech\\spark-nlp-book\\.env\\win\\lib\\site-packages\\sparknlp\\pretrained.py\u001b[0m in \u001b[0;36mdownloadPipeline\u001b[1;34m(name, language, remote_loc)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdownloadPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" download started this may take some time.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mfile_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_internal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_GetResourceSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfile_size\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"-1\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can not find the model to download please check the name!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mP:\\Dev\\Personal\\FinTech\\spark-nlp-book\\.env\\win\\lib\\site-packages\\sparknlp\\internal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, language, remote_loc)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         super(_GetResourceSize, self).__init__(\n\u001b[1;32m--> 232\u001b[1;33m             \"com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.getDownloadSize\", name, language, remote_loc)\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mP:\\Dev\\Personal\\FinTech\\spark-nlp-book\\.env\\win\\lib\\site-packages\\sparknlp\\internal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, java_obj, *args)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mExtendedJavaWrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_java_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mP:\\Dev\\Personal\\FinTech\\spark-nlp-book\\.env\\win\\lib\\site-packages\\sparknlp\\internal.py\u001b[0m in \u001b[0;36mnew_java_obj\u001b[1;34m(self, java_class, *args)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnew_java_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnew_java_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpylist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mP:\\Dev\\Personal\\FinTech\\spark-nlp-book\\.env\\win\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[1;34m(java_class, *args)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mjava_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mP:\\Dev\\Personal\\FinTech\\spark-nlp-book\\.env\\win\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mP:\\Dev\\Personal\\FinTech\\spark-nlp-book\\.env\\win\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mP:\\Dev\\Personal\\FinTech\\spark-nlp-book\\.env\\win\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.getDownloadSize.\n: java.lang.NoClassDefFoundError: org/json4s/package$MappingException\r\n\tat org.json4s.ext.EnumNameSerializer.deserialize(EnumSerializer.scala:53)\r\n\tat org.json4s.Formats$$anonfun$customDeserializer$1.applyOrElse(Formats.scala:66)\r\n\tat org.json4s.Formats$$anonfun$customDeserializer$1.applyOrElse(Formats.scala:66)\r\n\tat scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:180)\r\n\tat scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:167)\r\n\tat scala.collection.AbstractTraversable.collectFirst(Traversable.scala:108)\r\n\tat org.json4s.Formats$.customDeserializer(Formats.scala:66)\r\n\tat org.json4s.Extraction$.customOrElse(Extraction.scala:775)\r\n\tat org.json4s.Extraction$.extract(Extraction.scala:454)\r\n\tat org.json4s.Extraction$.extract(Extraction.scala:56)\r\n\tat org.json4s.ExtractableJsonAstNode.extract(ExtractableJsonAstNode.scala:22)\r\n\tat com.johnsnowlabs.util.JsonParser$.parseObject(JsonParser.scala:28)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceMetadata$.parseJson(ResourceMetadata.scala:109)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceMetadata$$anonfun$readResources$1.applyOrElse(ResourceMetadata.scala:138)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceMetadata$$anonfun$readResources$1.applyOrElse(ResourceMetadata.scala:137)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat scala.collection.Iterator$$anon$13.next(Iterator.scala:593)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:184)\r\n\tat scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:47)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toList(TraversableOnce.scala:350)\r\n\tat scala.collection.TraversableOnce.toList$(TraversableOnce.scala:350)\r\n\tat scala.collection.AbstractIterator.toList(Iterator.scala:1431)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceMetadata$.readResources(ResourceMetadata.scala:137)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceMetadata$.readResources(ResourceMetadata.scala:132)\r\n\tat com.johnsnowlabs.client.aws.AWSGateway.getMetadata(AWSGateway.scala:78)\r\n\tat com.johnsnowlabs.nlp.pretrained.S3ResourceDownloader.downloadMetadataIfNeed(S3ResourceDownloader.scala:62)\r\n\tat com.johnsnowlabs.nlp.pretrained.S3ResourceDownloader.resolveLink(S3ResourceDownloader.scala:68)\r\n\tat com.johnsnowlabs.nlp.pretrained.S3ResourceDownloader.getDownloadSize(S3ResourceDownloader.scala:145)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.getDownloadSize(ResourceDownloader.scala:445)\r\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader$.getDownloadSize(ResourceDownloader.scala:585)\r\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.getDownloadSize(ResourceDownloader.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.ClassNotFoundException: org.json4s.package$MappingException\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\t... 51 more\r\n"
     ]
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline('explain_document_ml', lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RV3MO_awABd"
   },
   "source": [
    "<p>The&nbsp;<code>.annotate()</code>&nbsp;method of the&nbsp;<code>BasicPipeline</code>&nbsp;can be used to annotate singular strings, as well as&nbsp;<code>DataFrame</code>s. Let's look at what it produces.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "k_s-B0Bhv-f5",
    "outputId": "78710a7c-0fe4-46c1-85cb-29cb83237064"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': ['Hellu wrold!'],\n",
       " 'lemmas': ['Hula', 'world', '!'],\n",
       " 'pos': ['NNP', 'NN', '.'],\n",
       " 'sentence': ['Hellu wrold!'],\n",
       " 'spell': ['Hula', 'world', '!'],\n",
       " 'stems': ['hula', 'world', '!'],\n",
       " 'token': ['Hellu', 'wrold', '!']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.annotate('Hellu wrold!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hokzQ2P6wHJu"
   },
   "source": [
    "<p>This a good amount of additional information. This brings up something that you will want to keep in mind - annotations can produce a lot of extra data. Let's look at the schema of the raw data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "wycwTIi9wBbA",
    "outputId": "ba6e6999-cdc9-42db-915a-5ae8391d2bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filename: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- newsgroup: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_wIrzugwKyn"
   },
   "source": [
    "<p>Now, let's annotate our&nbsp;<code>DataFrame</code>&nbsp;and look at the new schema.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AD-rGMgwcqS"
   },
   "outputs": [],
   "source": [
    "procd_texts_df = pipeline.annotate(texts_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VedrnReJwdaA",
    "outputId": "69e8c70a-2d90-4f7b-9205-ea8115f01215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filename: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- newsgroup: string (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- sentence: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- spell: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- lemmas: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- stems: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- pos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "procd_texts_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LT0pA0RrwPUq"
   },
   "source": [
    "<p>That is quite complex! To break it down, let's look at the token column. It has an&nbsp;<code>Array</code>&nbsp;type column, and each element is a&nbsp;<code>Struct</code>. Each element has the following</p>\n",
    "\n",
    "<ul>\n",
    "\t<li><code>annotatorType</code>: the type of annotation</li>\n",
    "\t<li><code>begin</code>: the starting character position of the annotation</li>\n",
    "\t<li><code>end</code>: the character position after the end of the annotation</li>\n",
    "\t<li><code>result</code>: the output of the annotator</li>\n",
    "\t<li><code>metadata</code>: a&nbsp;<code>Map</code>&nbsp;from&nbsp;<code>String</code>&nbsp;to&nbsp;<code>String</code>&nbsp;containing additional, potentially helpful, information about the annotation</li>\n",
    "</ul>\n",
    "\n",
    "<p>Let's look at some of the data using&nbsp;<code>.show()</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "xuIG7PyFwIqK",
    "outputId": "ee85c866-477f-4b60-fd4c-0331ec447591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            filename|                text|  newsgroup|            document|            sentence|               token|               spell|              lemmas|               stems|                 pos|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|file:/home/alex/p...|Xref: cantaloupe....|alt.atheism|[[document, 0, 13...|[[document, 0, 40...|[[token, 0, 3, Xr...|[[token, 0, 3, tr...|[[token, 0, 3, tr...|[[token, 0, 3, xr...|[[pos, 0, 3, NN, ...|\n",
      "|file:/home/alex/p...|Path: cantaloupe....|alt.atheism|[[document, 0, 99...|[[document, 0, 86...|[[token, 0, 3, Pa...|[[token, 0, 3, Pa...|[[token, 0, 3, Pa...|[[token, 0, 3, pa...|[[pos, 0, 3, NNP,...|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "procd_texts_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUzEnugUwur5"
   },
   "source": [
    "<p>This is not very readable. Not only is the automatic formatting doing poorly with this data, but we can hardly see our annotations. Let's try using some other arguments.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "colab_type": "code",
    "id": "F07fzZ-Nwh3e",
    "outputId": "2a6076a6-b5f6-4f06-ed0b-18ce56f4071f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------\n",
      " filename  | file:/home/alex/projects/spark-nlp-book-prod/jupyter/data/mini_newsgroups/alt.atheism/51121          \n",
      " text      | Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:51121 soc.motss:139944 rec.scouting:5318\n",
      "Newsgroups: ... \n",
      " newsgroup | alt.atheism                                                                                          \n",
      " document  | [[document, 0, 1346, Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:51121 soc.motss:139944 rec.scout... \n",
      " sentence  | [[document, 0, 407, Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:51121 soc.motss:139944 rec.scouti... \n",
      " token     | [[token, 0, 3, Xref, [sentence -> 0], []], [token, 4, 4, :, [sentence -> 0], []], [token, 6, 30, ... \n",
      " spell     | [[token, 0, 3, pref, [confidence -> 0.3333333333333333, sentence -> 0], []], [token, 4, 4, :, [co... \n",
      " lemmas    | [[token, 0, 3, xref, [confidence -> 0.3333333333333333, sentence -> 0], []], [token, 4, 4, :, [co... \n",
      " stems     | [[token, 0, 3, xref, [confidence -> 0.3333333333333333, sentence -> 0], []], [token, 4, 4, :, [co... \n",
      " pos       | [[pos, 0, 3, NN, [word -> xref], []], [pos, 4, 4, :, [word -> :], []], [pos, 6, 30, NN, [word -> ... \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------\n",
      " filename  | file:/home/alex/projects/spark-nlp-book-prod/jupyter/data/mini_newsgroups/alt.atheism/51126          \n",
      " text      | Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!ho... \n",
      " newsgroup | alt.atheism                                                                                          \n",
      " document  | [[document, 0, 996, Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!euro... \n",
      " sentence  | [[document, 0, 869, Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!euro... \n",
      " token     | [[token, 0, 3, Path, [sentence -> 0], []], [token, 4, 4, :, [sentence -> 0], []], [token, 6, 229,... \n",
      " spell     | [[token, 0, 3, Path, [confidence -> 1.0, sentence -> 0], []], [token, 4, 4, :, [confidence -> 0.0... \n",
      " lemmas    | [[token, 0, 3, Path, [confidence -> 1.0, sentence -> 0], []], [token, 4, 4, :, [confidence -> 0.0... \n",
      " stems     | [[token, 0, 3, path, [confidence -> 1.0, sentence -> 0], []], [token, 4, 4, :, [confidence -> 0.0... \n",
      " pos       | [[pos, 0, 3, NNP, [word -> Path], []], [pos, 4, 4, :, [word -> :], []], [pos, 6, 229, NN, [word -... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "procd_texts_df.show(n=2, truncate=100, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQlh5f33wyeN"
   },
   "source": [
    "<p>Better, but this is still not useful for getting a general understanding of our corpus. We at least have a glimpse of what our pipeline is doing.</p>\n",
    "\n",
    "<p>Now, we need to pull out the information we might want to use in other process, that is why there is the&nbsp;<code>Finisher</code>&nbsp;<code>Transformer</code>. The&nbsp;<code>Finisher</code>&nbsp;takes annotations and pulls out the pieces of data that we will be using in downstream processes. This allows us to use the results of our NLP pipeline in generic Spark. For now, let's pull out all the lemmas and put them into a&nbsp;<code>String</code>&nbsp;seperated by spaces.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "0kfj966Jww3x",
    "outputId": "4e47d3cf-0693-4d52-9065-650f309123bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------\n",
      " filename        | file:/home/alex/projects/spark-nlp-book-prod/jupyter/data/mini_newsgroups/alt.atheism/51121          \n",
      " text            | Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:51121 soc.motss:139944 rec.scouting:5318\n",
      "Newsgroups: ... \n",
      " newsgroup       | alt.atheism                                                                                          \n",
      " finished_lemmas | [tref, :, cantaloupe.srv.cs.cmu.edu, alt.atheism:51121, soc.motss:139944, rec.scouting:5318, News... \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp import Finisher\n",
    "finisher = Finisher()\n",
    "finisher = finisher\n",
    "# taking the lemma column\n",
    "finisher = finisher.setInputCols(['lemmas'])\n",
    "# seperating lemmas by a single space\n",
    "finisher = finisher.setAnnotationSplitSymbol(' ')\n",
    "finished_texts_df = finisher.transform(procd_texts_df)\n",
    "finished_texts_df.show(n=1, truncate=100, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpa9aXT4w2xU"
   },
   "source": [
    "<p>Normally, we'll be using the&nbsp;<code>.setOutputAsArray(True)</code>&nbsp;option so that the output is an&nbsp;<code>Array</code>instead of a&nbsp;<code>String</code>.</p>\n",
    "\n",
    "<p>Let's look at the final result on the first document.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8IQSc5hRw1Op",
    "outputId": "8dc11c72-4a29-4207-e778-dc488e2449b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(finished_lemmas=['pref', ':', 'cantaloupe.srv.cs.cmu.edu', 'alt.atheism:51121', 'soc.motss:139944', 'rec.scouting:5318', 'Newsgroups', ':', 'alt.atheism,soc.motss,rec.scouting', 'Path', ':', 'cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!howland.reston.ans.net!wupost!uunet!newsgate.watson.ibm.com!yktnews.watson.ibm.com!watson!Watson.Ibm.Com!strom', 'From', ':', 'strom@Watson.Ibm.Com', '(', 'Rob', 'Strom', ')', 'Subject', ':', 'Re', ':', '[soc.motss', ',', 'et', 'al', '.', ']', '\"', 'Princeton', 'ax', 'match', 'fund', 'for', 'Boy', 'Scouts', '\"', 'Sender', ':', '@watson.ibm.com', 'Message-ID', ':', '<1993Apr05.180116.43346@watson.ibm.com>', 'Date', ':', 'Mon', ',', '05', 'Apr', '93', '18:01:16', 'GMT', 'Distribution', ':', 'usa', 'References', ':', '<C47EFs.3q47@austin.ibm.com>', '<1993Mar22.033150.17345@cbnewsl.cb.att.com>', '<N4HY.93Apr5120934@harder.ccr-p.ida.org>', 'Organization', ':', 'IBM', 'Research', 'Lines', ':', '15', 'In', 'article', '<N4HY.93Apr5120934@harder.ccr-p.ida.org>', ',', 'n4hy@harder.ccr-p.ida.org', '(', 'Bob', 'McGwier', ')', 'write', ':', '|>', '[1]', 'HOWEVER', ',', 'I', 'hate', 'economic', 'terrorism', 'and', 'political', 'correctness', '|>', 'bad', 'than', 'I', 'hate', 'this', 'policy', '.', '|>', '[2]', 'A', 'more', 'effective', 'approach', 'be', 'to', 'stop', 'donate', '|>', 'to', 'ANY', 'organizating', 'that', 'directly', 'or', 'indirectly', 'support', 'gay', 'right', 'issue', '|>', 'until', 'they', 'end', 'the', 'boycott', 'on', 'fund', 'of', 'scout', '.', 'Can', 'somebody', 'reconcile', 'the', 'apparent', 'contradiction', 'between', '[1]', 'and', '[2]', '?', '--', 'Rob', 'Strom', ',', 'strom@watson.ibm.com', ',', '(', '914', ')', '784-7641', 'IBM', 'Research', ',', '30', 'Saw', 'Mill', 'River', 'Road', ',', 'P.O', '.', 'Box', '704', ',', 'Yorktown', 'Heights', ',', 'NY', '10598'])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished_texts_df.select('finished_lemmas').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNG2mFVSyB28"
   },
   "source": [
    "<p>It doesn't look like much has been done here, but there is still a lot to unpack. In the next chapter, we will explain some basics of linguistics that will help us understand what these annotators are doing.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"1.1_Getting_Started.html4\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"deployment in production environment\" data-see=\"productionizing NLP applications\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"emotion detection\" data-see=\"sentiment analysis and emotion detection\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"Keras\" data-see=\"sequence modeling with Keras\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"labeling, human\" data-see=\"human labeling\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"libraries\" data-see=\"specific libraries\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"natural language processing (NLP) basics\" data-secondary=\"libraries\" data-see=\"NLP libraries\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"paragraph2vec\" data-see=\"doc2vec\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"publishing\" data-see=\"productionizing NLP applications\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"shingles\" data-see=\"N-grams\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"Spark\" data-see=\"Apache Spark\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"Spark MLlib\" data-see=\"MLlib\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"text\" data-see=\"human labeling; object character recognition\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"wikis\" data-see=\"knowledge bases\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"CNNs\" data-see=\"convolutional neural networks\" data-type=\"indexterm\">&nbsp;</a><a contenteditable=\"false\" data-primary=\"RNNs\" data-see=\"recurrent neural networks\" data-type=\"indexterm\">&nbsp;</a></p>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNWIyxHwf4Qjj+EW7oss08O",
   "include_colab_link": true,
   "name": "1.1_Getting_Started",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
